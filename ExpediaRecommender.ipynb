{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c014c58e-004b-4a9b-9425-4916385b9b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "\n",
    "import csv\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37813262",
   "metadata": {},
   "source": [
    "## Load the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7ca6fd-7dfc-4941-b931-edb70991cdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and test datasets\n",
    "training_set = 'dmt-2025-2nd-assignment/training_set_VU_DM.csv'\n",
    "test_set = 'dmt-2025-2nd-assignment/test_set_VU_DM.csv'\n",
    "\n",
    "training_set = pd.read_csv(training_set)\n",
    "test_set = pd.read_csv(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc97c10b-e708-4173-9aae-b4cf2ae4a5aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>srch_id</th>\n",
       "      <th>date_time</th>\n",
       "      <th>site_id</th>\n",
       "      <th>visitor_location_country_id</th>\n",
       "      <th>visitor_hist_starrating</th>\n",
       "      <th>visitor_hist_adr_usd</th>\n",
       "      <th>prop_country_id</th>\n",
       "      <th>prop_id</th>\n",
       "      <th>prop_starrating</th>\n",
       "      <th>prop_review_score</th>\n",
       "      <th>...</th>\n",
       "      <th>comp6_rate_percent_diff</th>\n",
       "      <th>comp7_rate</th>\n",
       "      <th>comp7_inv</th>\n",
       "      <th>comp7_rate_percent_diff</th>\n",
       "      <th>comp8_rate</th>\n",
       "      <th>comp8_inv</th>\n",
       "      <th>comp8_rate_percent_diff</th>\n",
       "      <th>click_bool</th>\n",
       "      <th>gross_bookings_usd</th>\n",
       "      <th>booking_bool</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2013-04-04 08:32:15</td>\n",
       "      <td>12</td>\n",
       "      <td>187</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>219</td>\n",
       "      <td>893</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2013-04-04 08:32:15</td>\n",
       "      <td>12</td>\n",
       "      <td>187</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>219</td>\n",
       "      <td>10404</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2013-04-04 08:32:15</td>\n",
       "      <td>12</td>\n",
       "      <td>187</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>219</td>\n",
       "      <td>21315</td>\n",
       "      <td>3</td>\n",
       "      <td>4.5</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2013-04-04 08:32:15</td>\n",
       "      <td>12</td>\n",
       "      <td>187</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>219</td>\n",
       "      <td>27348</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2013-04-04 08:32:15</td>\n",
       "      <td>12</td>\n",
       "      <td>187</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>219</td>\n",
       "      <td>29604</td>\n",
       "      <td>4</td>\n",
       "      <td>3.5</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   srch_id            date_time  site_id  visitor_location_country_id  \\\n",
       "0        1  2013-04-04 08:32:15       12                          187   \n",
       "1        1  2013-04-04 08:32:15       12                          187   \n",
       "2        1  2013-04-04 08:32:15       12                          187   \n",
       "3        1  2013-04-04 08:32:15       12                          187   \n",
       "4        1  2013-04-04 08:32:15       12                          187   \n",
       "\n",
       "   visitor_hist_starrating  visitor_hist_adr_usd  prop_country_id  prop_id  \\\n",
       "0                      NaN                   NaN              219      893   \n",
       "1                      NaN                   NaN              219    10404   \n",
       "2                      NaN                   NaN              219    21315   \n",
       "3                      NaN                   NaN              219    27348   \n",
       "4                      NaN                   NaN              219    29604   \n",
       "\n",
       "   prop_starrating  prop_review_score  ...  comp6_rate_percent_diff  \\\n",
       "0                3                3.5  ...                      NaN   \n",
       "1                4                4.0  ...                      NaN   \n",
       "2                3                4.5  ...                      NaN   \n",
       "3                2                4.0  ...                      NaN   \n",
       "4                4                3.5  ...                      NaN   \n",
       "\n",
       "   comp7_rate  comp7_inv  comp7_rate_percent_diff  comp8_rate  comp8_inv  \\\n",
       "0         NaN        NaN                      NaN         0.0        0.0   \n",
       "1         NaN        NaN                      NaN         0.0        0.0   \n",
       "2         NaN        NaN                      NaN         0.0        0.0   \n",
       "3         NaN        NaN                      NaN        -1.0        0.0   \n",
       "4         NaN        NaN                      NaN         0.0        0.0   \n",
       "\n",
       "   comp8_rate_percent_diff  click_bool  gross_bookings_usd  booking_bool  \n",
       "0                      NaN           0                 NaN             0  \n",
       "1                      NaN           0                 NaN             0  \n",
       "2                      NaN           0                 NaN             0  \n",
       "3                      5.0           0                 NaN             0  \n",
       "4                      NaN           0                 NaN             0  \n",
       "\n",
       "[5 rows x 54 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea5fae3",
   "metadata": {},
   "source": [
    "The datafields can be interpreted according to the following scheme: https://www.kaggle.com/c/expedia-personalized-sort/data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78549ff8",
   "metadata": {},
   "source": [
    "## Explorotary Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cfbac23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (4958347, 54)\n",
      "Test set shape: (4959183, 50)\n",
      "First few rows of the training set:\n",
      "   srch_id            date_time  site_id  visitor_location_country_id  \\\n",
      "0        1  2013-04-04 08:32:15       12                          187   \n",
      "1        1  2013-04-04 08:32:15       12                          187   \n",
      "2        1  2013-04-04 08:32:15       12                          187   \n",
      "3        1  2013-04-04 08:32:15       12                          187   \n",
      "4        1  2013-04-04 08:32:15       12                          187   \n",
      "\n",
      "   visitor_hist_starrating  visitor_hist_adr_usd  prop_country_id  prop_id  \\\n",
      "0                      NaN                   NaN              219      893   \n",
      "1                      NaN                   NaN              219    10404   \n",
      "2                      NaN                   NaN              219    21315   \n",
      "3                      NaN                   NaN              219    27348   \n",
      "4                      NaN                   NaN              219    29604   \n",
      "\n",
      "   prop_starrating  prop_review_score  ...  comp6_rate_percent_diff  \\\n",
      "0                3                3.5  ...                      NaN   \n",
      "1                4                4.0  ...                      NaN   \n",
      "2                3                4.5  ...                      NaN   \n",
      "3                2                4.0  ...                      NaN   \n",
      "4                4                3.5  ...                      NaN   \n",
      "\n",
      "   comp7_rate  comp7_inv  comp7_rate_percent_diff  comp8_rate  comp8_inv  \\\n",
      "0         NaN        NaN                      NaN         0.0        0.0   \n",
      "1         NaN        NaN                      NaN         0.0        0.0   \n",
      "2         NaN        NaN                      NaN         0.0        0.0   \n",
      "3         NaN        NaN                      NaN        -1.0        0.0   \n",
      "4         NaN        NaN                      NaN         0.0        0.0   \n",
      "\n",
      "   comp8_rate_percent_diff  click_bool  gross_bookings_usd  booking_bool  \n",
      "0                      NaN           0                 NaN             0  \n",
      "1                      NaN           0                 NaN             0  \n",
      "2                      NaN           0                 NaN             0  \n",
      "3                      5.0           0                 NaN             0  \n",
      "4                      NaN           0                 NaN             0  \n",
      "\n",
      "[5 rows x 54 columns]\n",
      "Columns in the training set:\n",
      "Index(['srch_id', 'date_time', 'site_id', 'visitor_location_country_id',\n",
      "       'visitor_hist_starrating', 'visitor_hist_adr_usd', 'prop_country_id',\n",
      "       'prop_id', 'prop_starrating', 'prop_review_score', 'prop_brand_bool',\n",
      "       'prop_location_score1', 'prop_location_score2',\n",
      "       'prop_log_historical_price', 'position', 'price_usd', 'promotion_flag',\n",
      "       'srch_destination_id', 'srch_length_of_stay', 'srch_booking_window',\n",
      "       'srch_adults_count', 'srch_children_count', 'srch_room_count',\n",
      "       'srch_saturday_night_bool', 'srch_query_affinity_score',\n",
      "       'orig_destination_distance', 'random_bool', 'comp1_rate', 'comp1_inv',\n",
      "       'comp1_rate_percent_diff', 'comp2_rate', 'comp2_inv',\n",
      "       'comp2_rate_percent_diff', 'comp3_rate', 'comp3_inv',\n",
      "       'comp3_rate_percent_diff', 'comp4_rate', 'comp4_inv',\n",
      "       'comp4_rate_percent_diff', 'comp5_rate', 'comp5_inv',\n",
      "       'comp5_rate_percent_diff', 'comp6_rate', 'comp6_inv',\n",
      "       'comp6_rate_percent_diff', 'comp7_rate', 'comp7_inv',\n",
      "       'comp7_rate_percent_diff', 'comp8_rate', 'comp8_inv',\n",
      "       'comp8_rate_percent_diff', 'click_bool', 'gross_bookings_usd',\n",
      "       'booking_bool'],\n",
      "      dtype='object')\n",
      "Columns in the test set:\n",
      "Index(['srch_id', 'date_time', 'site_id', 'visitor_location_country_id',\n",
      "       'visitor_hist_starrating', 'visitor_hist_adr_usd', 'prop_country_id',\n",
      "       'prop_id', 'prop_starrating', 'prop_review_score', 'prop_brand_bool',\n",
      "       'prop_location_score1', 'prop_location_score2',\n",
      "       'prop_log_historical_price', 'price_usd', 'promotion_flag',\n",
      "       'srch_destination_id', 'srch_length_of_stay', 'srch_booking_window',\n",
      "       'srch_adults_count', 'srch_children_count', 'srch_room_count',\n",
      "       'srch_saturday_night_bool', 'srch_query_affinity_score',\n",
      "       'orig_destination_distance', 'random_bool', 'comp1_rate', 'comp1_inv',\n",
      "       'comp1_rate_percent_diff', 'comp2_rate', 'comp2_inv',\n",
      "       'comp2_rate_percent_diff', 'comp3_rate', 'comp3_inv',\n",
      "       'comp3_rate_percent_diff', 'comp4_rate', 'comp4_inv',\n",
      "       'comp4_rate_percent_diff', 'comp5_rate', 'comp5_inv',\n",
      "       'comp5_rate_percent_diff', 'comp6_rate', 'comp6_inv',\n",
      "       'comp6_rate_percent_diff', 'comp7_rate', 'comp7_inv',\n",
      "       'comp7_rate_percent_diff', 'comp8_rate', 'comp8_inv',\n",
      "       'comp8_rate_percent_diff'],\n",
      "      dtype='object')\n",
      "Data types of the columns in the training set:\n",
      "srch_id                          int64\n",
      "date_time                       object\n",
      "site_id                          int64\n",
      "visitor_location_country_id      int64\n",
      "visitor_hist_starrating        float64\n",
      "visitor_hist_adr_usd           float64\n",
      "prop_country_id                  int64\n",
      "prop_id                          int64\n",
      "prop_starrating                  int64\n",
      "prop_review_score              float64\n",
      "prop_brand_bool                  int64\n",
      "prop_location_score1           float64\n",
      "prop_location_score2           float64\n",
      "prop_log_historical_price      float64\n",
      "position                         int64\n",
      "price_usd                      float64\n",
      "promotion_flag                   int64\n",
      "srch_destination_id              int64\n",
      "srch_length_of_stay              int64\n",
      "srch_booking_window              int64\n",
      "srch_adults_count                int64\n",
      "srch_children_count              int64\n",
      "srch_room_count                  int64\n",
      "srch_saturday_night_bool         int64\n",
      "srch_query_affinity_score      float64\n",
      "orig_destination_distance      float64\n",
      "random_bool                      int64\n",
      "comp1_rate                     float64\n",
      "comp1_inv                      float64\n",
      "comp1_rate_percent_diff        float64\n",
      "comp2_rate                     float64\n",
      "comp2_inv                      float64\n",
      "comp2_rate_percent_diff        float64\n",
      "comp3_rate                     float64\n",
      "comp3_inv                      float64\n",
      "comp3_rate_percent_diff        float64\n",
      "comp4_rate                     float64\n",
      "comp4_inv                      float64\n",
      "comp4_rate_percent_diff        float64\n",
      "comp5_rate                     float64\n",
      "comp5_inv                      float64\n",
      "comp5_rate_percent_diff        float64\n",
      "comp6_rate                     float64\n",
      "comp6_inv                      float64\n",
      "comp6_rate_percent_diff        float64\n",
      "comp7_rate                     float64\n",
      "comp7_inv                      float64\n",
      "comp7_rate_percent_diff        float64\n",
      "comp8_rate                     float64\n",
      "comp8_inv                      float64\n",
      "comp8_rate_percent_diff        float64\n",
      "click_bool                       int64\n",
      "gross_bookings_usd             float64\n",
      "booking_bool                     int64\n",
      "dtype: object\n",
      "Data types of the columns in the test set:\n",
      "srch_id                          int64\n",
      "date_time                       object\n",
      "site_id                          int64\n",
      "visitor_location_country_id      int64\n",
      "visitor_hist_starrating        float64\n",
      "visitor_hist_adr_usd           float64\n",
      "prop_country_id                  int64\n",
      "prop_id                          int64\n",
      "prop_starrating                  int64\n",
      "prop_review_score              float64\n",
      "prop_brand_bool                  int64\n",
      "prop_location_score1           float64\n",
      "prop_location_score2           float64\n",
      "prop_log_historical_price      float64\n",
      "price_usd                      float64\n",
      "promotion_flag                   int64\n",
      "srch_destination_id              int64\n",
      "srch_length_of_stay              int64\n",
      "srch_booking_window              int64\n",
      "srch_adults_count                int64\n",
      "srch_children_count              int64\n",
      "srch_room_count                  int64\n",
      "srch_saturday_night_bool         int64\n",
      "srch_query_affinity_score      float64\n",
      "orig_destination_distance      float64\n",
      "random_bool                      int64\n",
      "comp1_rate                     float64\n",
      "comp1_inv                      float64\n",
      "comp1_rate_percent_diff        float64\n",
      "comp2_rate                     float64\n",
      "comp2_inv                      float64\n",
      "comp2_rate_percent_diff        float64\n",
      "comp3_rate                     float64\n",
      "comp3_inv                      float64\n",
      "comp3_rate_percent_diff        float64\n",
      "comp4_rate                     float64\n",
      "comp4_inv                      float64\n",
      "comp4_rate_percent_diff        float64\n",
      "comp5_rate                     float64\n",
      "comp5_inv                      float64\n",
      "comp5_rate_percent_diff        float64\n",
      "comp6_rate                     float64\n",
      "comp6_inv                      float64\n",
      "comp6_rate_percent_diff        float64\n",
      "comp7_rate                     float64\n",
      "comp7_inv                      float64\n",
      "comp7_rate_percent_diff        float64\n",
      "comp8_rate                     float64\n",
      "comp8_inv                      float64\n",
      "comp8_rate_percent_diff        float64\n",
      "dtype: object\n",
      "Missing values in the training set:\n",
      "srch_id                              0\n",
      "date_time                            0\n",
      "site_id                              0\n",
      "visitor_location_country_id          0\n",
      "visitor_hist_starrating        4706481\n",
      "visitor_hist_adr_usd           4705359\n",
      "prop_country_id                      0\n",
      "prop_id                              0\n",
      "prop_starrating                      0\n",
      "prop_review_score                 7364\n",
      "prop_brand_bool                      0\n",
      "prop_location_score1                 0\n",
      "prop_location_score2           1090348\n",
      "prop_log_historical_price            0\n",
      "position                             0\n",
      "price_usd                            0\n",
      "promotion_flag                       0\n",
      "srch_destination_id                  0\n",
      "srch_length_of_stay                  0\n",
      "srch_booking_window                  0\n",
      "srch_adults_count                    0\n",
      "srch_children_count                  0\n",
      "srch_room_count                      0\n",
      "srch_saturday_night_bool             0\n",
      "srch_query_affinity_score      4640941\n",
      "orig_destination_distance      1607782\n",
      "random_bool                          0\n",
      "comp1_rate                     4838417\n",
      "comp1_inv                      4828788\n",
      "comp1_rate_percent_diff        4863908\n",
      "comp2_rate                     2933675\n",
      "comp2_inv                      2828078\n",
      "comp2_rate_percent_diff        4402109\n",
      "comp3_rate                     3424059\n",
      "comp3_inv                      3307357\n",
      "comp3_rate_percent_diff        4485550\n",
      "comp4_rate                     4650969\n",
      "comp4_inv                      4614684\n",
      "comp4_rate_percent_diff        4827261\n",
      "comp5_rate                     2735974\n",
      "comp5_inv                      2598327\n",
      "comp5_rate_percent_diff        4117248\n",
      "comp6_rate                     4718190\n",
      "comp6_inv                      4697371\n",
      "comp6_rate_percent_diff        4862173\n",
      "comp7_rate                     4642999\n",
      "comp7_inv                      4601925\n",
      "comp7_rate_percent_diff        4819832\n",
      "comp8_rate                     3041693\n",
      "comp8_inv                      2970844\n",
      "comp8_rate_percent_diff        4343617\n",
      "click_bool                           0\n",
      "gross_bookings_usd             4819957\n",
      "booking_bool                         0\n",
      "dtype: int64\n",
      "Missing values in the test set:\n",
      "srch_id                              0\n",
      "date_time                            0\n",
      "site_id                              0\n",
      "visitor_location_country_id          0\n",
      "visitor_hist_starrating        4705752\n",
      "visitor_hist_adr_usd           4704559\n",
      "prop_country_id                      0\n",
      "prop_id                              0\n",
      "prop_starrating                      0\n",
      "prop_review_score                 7266\n",
      "prop_brand_bool                      0\n",
      "prop_location_score1                 0\n",
      "prop_location_score2           1088032\n",
      "prop_log_historical_price            0\n",
      "price_usd                            0\n",
      "promotion_flag                       0\n",
      "srch_destination_id                  0\n",
      "srch_length_of_stay                  0\n",
      "srch_booking_window                  0\n",
      "srch_adults_count                    0\n",
      "srch_children_count                  0\n",
      "srch_room_count                      0\n",
      "srch_saturday_night_bool             0\n",
      "srch_query_affinity_score      4641025\n",
      "orig_destination_distance      1608679\n",
      "random_bool                          0\n",
      "comp1_rate                     4843307\n",
      "comp1_inv                      4834309\n",
      "comp1_rate_percent_diff        4868715\n",
      "comp2_rate                     2943222\n",
      "comp2_inv                      2837914\n",
      "comp2_rate_percent_diff        4405574\n",
      "comp3_rate                     3434198\n",
      "comp3_inv                      3317952\n",
      "comp3_rate_percent_diff        4487973\n",
      "comp4_rate                     4646462\n",
      "comp4_inv                      4610375\n",
      "comp4_rate_percent_diff        4826056\n",
      "comp5_rate                     2737262\n",
      "comp5_inv                      2598370\n",
      "comp5_rate_percent_diff        4119276\n",
      "comp6_rate                     4716853\n",
      "comp6_inv                      4696014\n",
      "comp6_rate_percent_diff        4862045\n",
      "comp7_rate                     4643454\n",
      "comp7_inv                      4602430\n",
      "comp7_rate_percent_diff        4819860\n",
      "comp8_rate                     3056794\n",
      "comp8_inv                      2986298\n",
      "comp8_rate_percent_diff        4348206\n",
      "dtype: int64\n",
      "Distribution of the target variable in the training set:\n",
      "prop_id\n",
      "104517    2357\n",
      "124342    2355\n",
      "68420     2285\n",
      "40279     2275\n",
      "134154    2257\n",
      "          ... \n",
      "115439       1\n",
      "134795       1\n",
      "57042        1\n",
      "66680        1\n",
      "87173        1\n",
      "Name: count, Length: 129113, dtype: int64\n",
      "Distribution of the target variable in the test set:\n",
      "prop_id\n",
      "104517    2376\n",
      "124342    2352\n",
      "68420     2295\n",
      "134154    2293\n",
      "59781     2269\n",
      "          ... \n",
      "92909        1\n",
      "96455        1\n",
      "112837       1\n",
      "130395       1\n",
      "109037       1\n",
      "Name: count, Length: 129438, dtype: int64\n",
      "Unique values in the target variable in the training set:\n",
      "[   893  10404  21315 ... 135856   2284  87173]\n",
      "Unique values in the target variable in the test set:\n",
      "[  3180   5543  14142 ...  41332 102926 109037]\n"
     ]
    }
   ],
   "source": [
    "# Check the shape of the training and test datasets\n",
    "print(\"Training set shape:\", training_set.shape)\n",
    "print(\"Test set shape:\", test_set.shape)\n",
    "\n",
    "# Check the first few rows of the training set\n",
    "print(\"First few rows of the training set:\")\n",
    "print(training_set.head())\n",
    "\n",
    "# Check the columns of the training set\n",
    "print(\"Columns in the training set:\")\n",
    "print(training_set.columns)\n",
    "# Check the columns of the test set\n",
    "print(\"Columns in the test set:\")\n",
    "print(test_set.columns)\n",
    "\n",
    "# Check the data types of the columns in the training set\n",
    "print(\"Data types of the columns in the training set:\")\n",
    "print(training_set.dtypes)\n",
    "\n",
    "# Check the data types of the columns in the test set\n",
    "print(\"Data types of the columns in the test set:\")\n",
    "print(test_set.dtypes)\n",
    "\n",
    "# Check for missing values in the training set\n",
    "print(\"Missing values in the training set:\")\n",
    "print(training_set.isnull().sum())\n",
    "# Check for missing values in the test set\n",
    "print(\"Missing values in the test set:\")\n",
    "print(test_set.isnull().sum())\n",
    "\n",
    "# Check the distribution of the target variable in the training set\n",
    "print(\"Distribution of the target variable in the training set:\")\n",
    "print(training_set['prop_id'].value_counts())\n",
    "# Check the distribution of the target variable in the test set\n",
    "print(\"Distribution of the target variable in the test set:\")\n",
    "print(test_set['prop_id'].value_counts())\n",
    "\n",
    "# Check the unique values in the target variable in the training set\n",
    "print(\"Unique values in the target variable in the training set:\")\n",
    "print(training_set['prop_id'].unique())\n",
    "# Check the unique values in the target variable in the test set\n",
    "print(\"Unique values in the target variable in the test set:\")\n",
    "print(test_set['prop_id'].unique())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6aeb03",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "\n",
    "### Target value\n",
    "\n",
    "The target value is one or multiple property IDs for a given search ID.\n",
    "\n",
    "### Features\n",
    "\n",
    "Values not present in the test data (and thus not to be used as features) include: \n",
    "- `position`\n",
    "- `click_bool`\n",
    "- `gross_bookings_usd`\n",
    "- `booking_bool`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a51f2c",
   "metadata": {},
   "source": [
    "### Derive date features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61fb5ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few rows of the training set after feature extraction:\n",
      "   srch_id           date_time  site_id  visitor_location_country_id  \\\n",
      "0        1 2013-04-04 08:32:15       12                          187   \n",
      "1        1 2013-04-04 08:32:15       12                          187   \n",
      "2        1 2013-04-04 08:32:15       12                          187   \n",
      "3        1 2013-04-04 08:32:15       12                          187   \n",
      "4        1 2013-04-04 08:32:15       12                          187   \n",
      "\n",
      "   visitor_hist_starrating  visitor_hist_adr_usd  prop_country_id  prop_id  \\\n",
      "0                      NaN                   NaN              219      893   \n",
      "1                      NaN                   NaN              219    10404   \n",
      "2                      NaN                   NaN              219    21315   \n",
      "3                      NaN                   NaN              219    27348   \n",
      "4                      NaN                   NaN              219    29604   \n",
      "\n",
      "   prop_starrating  prop_review_score  ...  click_bool  gross_bookings_usd  \\\n",
      "0                3                3.5  ...           0                 NaN   \n",
      "1                4                4.0  ...           0                 NaN   \n",
      "2                3                4.5  ...           0                 NaN   \n",
      "3                2                4.0  ...           0                 NaN   \n",
      "4                4                3.5  ...           0                 NaN   \n",
      "\n",
      "   booking_bool  month  day  year  hour  season  day_of_week  is_weekend  \n",
      "0             0      4    4  2013     8       2            3           0  \n",
      "1             0      4    4  2013     8       2            3           0  \n",
      "2             0      4    4  2013     8       2            3           0  \n",
      "3             0      4    4  2013     8       2            3           0  \n",
      "4             0      4    4  2013     8       2            3           0  \n",
      "\n",
      "[5 rows x 61 columns]\n"
     ]
    }
   ],
   "source": [
    "# Convert the `date_time` column to:\n",
    "# Month, Day, Year, Hour, Season, Day of the week, Weekend\n",
    "def extract_date_features(df):\n",
    "    df['date_time'] = pd.to_datetime(df['date_time'])\n",
    "    df['month'] = df['date_time'].dt.month\n",
    "    df['day'] = df['date_time'].dt.day\n",
    "    df['year'] = df['date_time'].dt.year\n",
    "    df['hour'] = df['date_time'].dt.hour\n",
    "    df['season'] = (df['month'] % 12 + 3) // 3\n",
    "    df['day_of_week'] = df['date_time'].dt.dayofweek\n",
    "    df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)\n",
    "    return df\n",
    "\n",
    "# Apply the function to the training and test sets\n",
    "training_set = extract_date_features(training_set)\n",
    "test_set = extract_date_features(test_set)\n",
    "\n",
    "# Check the first few rows of the training set after feature extraction\n",
    "print(\"First few rows of the training set after feature extraction:\")\n",
    "print(training_set.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0f9b85",
   "metadata": {},
   "source": [
    "## Train baseline model\n",
    "\n",
    "Let's start with a LGBMRanker model using all feature (including the engineered features derived from the booking date)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1351b5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.263207 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3981\n",
      "[LightGBM] [Info] Number of data points in the train set: 4958347, number of used features: 54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/krisstallenberg/anaconda3/envs/recommender-expedia/lib/python3.10/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRanker was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to ranked_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from lightgbm import LGBMRanker\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Parameters\n",
    "training_fraction = 1\n",
    "use_validation = False\n",
    "\n",
    "# Define the features and target variable\n",
    "features = ['month', 'day', 'year', 'hour', 'season', 'day_of_week', 'is_weekend', 'site_id', 'visitor_location_country_id', 'visitor_hist_starrating', 'visitor_hist_adr_usd', 'prop_country_id', 'prop_starrating', 'prop_review_score', 'prop_brand_bool', 'prop_location_score1', 'prop_location_score2', 'prop_log_historical_price', 'price_usd', 'promotion_flag','srch_destination_id', 'srch_length_of_stay', 'srch_booking_window', 'srch_adults_count', 'srch_children_count', 'srch_room_count', 'srch_saturday_night_bool', 'srch_query_affinity_score', 'orig_destination_distance', 'random_bool', 'comp1_rate', 'comp1_inv', 'comp1_rate_percent_diff','comp2_rate', 'comp2_inv', 'comp2_rate_percent_diff', 'comp3_rate', 'comp3_inv', 'comp3_rate_percent_diff', 'comp4_rate', 'comp4_inv', 'comp4_rate_percent_diff', 'comp5_rate', 'comp5_inv', 'comp5_rate_percent_diff', 'comp6_rate', 'comp6_inv', 'comp6_rate_percent_diff', 'comp7_rate', 'comp7_inv', 'comp7_rate_percent_diff', 'comp8_rate', 'comp8_inv', 'comp8_rate_percent_diff']\n",
    "target = 'booking_bool'\n",
    "\n",
    "# Separate numeric and categorical features\n",
    "numeric_features = ['month', 'day', 'year', 'hour', 'visitor_hist_starrating', 'visitor_hist_adr_usd', 'prop_starrating', 'prop_review_score', 'prop_location_score1', 'prop_location_score2', 'prop_log_historical_price', 'price_usd', 'srch_length_of_stay', 'srch_booking_window', 'srch_adults_count', 'srch_children_count', 'srch_room_count', 'srch_query_affinity_score', 'orig_destination_distance']\n",
    "categorical_features = ['season', 'day_of_week', 'is_weekend', 'site_id', 'visitor_location_country_id', 'prop_country_id', 'prop_brand_bool', 'promotion_flag', 'srch_destination_id', 'srch_saturday_night_bool', 'random_bool', 'comp1_rate', 'comp1_inv', 'comp1_rate_percent_diff','comp2_rate', 'comp2_inv', 'comp2_rate_percent_diff', 'comp3_rate', 'comp3_inv', 'comp3_rate_percent_diff', 'comp4_rate', 'comp4_inv', 'comp4_rate_percent_diff', 'comp5_rate', 'comp5_inv', 'comp5_rate_percent_diff', 'comp6_rate', 'comp6_inv', 'comp6_rate_percent_diff', 'comp7_rate', 'comp7_inv', 'comp7_rate_percent_diff', 'comp8_rate', 'comp8_inv', 'comp8_rate_percent_diff']\n",
    "\n",
    "# Sample the training data\n",
    "training_sample = training_set.sample(frac=training_fraction, random_state=42)\n",
    "X = training_sample[features + ['srch_id']]\n",
    "y = training_sample[target]\n",
    "\n",
    "group = X.groupby('srch_id').size().to_list()\n",
    "X = X.drop(columns=['srch_id'])\n",
    "\n",
    "# Preprocessing\n",
    "numeric_transformer = make_pipeline(\n",
    "    SimpleImputer(strategy='mean'),\n",
    "    StandardScaler()\n",
    ")\n",
    "\n",
    "categorical_transformer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "preprocessing = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# LightGBM Ranker\n",
    "lgbm_model = LGBMRanker(\n",
    "    objective='lambdarank',\n",
    "    metric='ndcg',\n",
    "    learning_rate=0.1,\n",
    "    num_leaves=31,\n",
    "    max_depth=10,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Define the full pipeline with named steps\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessing', preprocessing),\n",
    "    ('lgbm', lgbm_model)\n",
    "])\n",
    "\n",
    "model.fit(X, y, lgbm__group=group)\n",
    "\n",
    "# Test prediction\n",
    "X_test = test_set[features]\n",
    "srch_ids = test_set['srch_id']\n",
    "prop_ids = test_set['prop_id']\n",
    "\n",
    "scores = model.predict(X_test)\n",
    "\n",
    "output_df = pd.DataFrame({\n",
    "    'srch_id': srch_ids,\n",
    "    'prop_id': prop_ids,\n",
    "    'score': scores\n",
    "})\n",
    "\n",
    "output_df = output_df.sort_values(by=['srch_id', 'score'], ascending=[True, False])\n",
    "output_df[['srch_id', 'prop_id']].to_csv('ranked_predictions.csv', index=False)\n",
    "print(\"Predictions saved to ranked_predictions.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ddd1fa",
   "metadata": {},
   "source": [
    "The baseline [scores 0.35831](https://www.kaggle.com/competitions/dmt-2025-2nd-assignment/leaderboard).\n",
    "\n",
    "The following is a first attempt at hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0eff9a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.266400 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3981\n",
      "[LightGBM] [Info] Number of data points in the train set: 4958347, number of used features: 54\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/krisstallenberg/anaconda3/envs/recommender-expedia/lib/python3.10/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRanker was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "Predictions saved to ranked_predictions.csv\n",
      "Best iteration: 0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from lightgbm import LGBMRanker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Split data\n",
    "if use_validation:\n",
    "    unique_ids = training_sample['srch_id'].unique()\n",
    "    val_ids = set(np.random.choice(unique_ids, size=int(0.2 * len(unique_ids)), replace=False))\n",
    "\n",
    "    train_idx = training_sample['srch_id'].isin(val_ids) == False\n",
    "    val_idx = training_sample['srch_id'].isin(val_ids)\n",
    "\n",
    "    train_set = training_sample[train_idx]\n",
    "    val_set = training_sample[val_idx]\n",
    "\n",
    "    X_train = train_set[features]\n",
    "    y_train = train_set[target]\n",
    "    group_train = train_set.groupby('srch_id').size().to_list()\n",
    "\n",
    "    X_val = val_set[features]\n",
    "    y_val = val_set[target]\n",
    "    group_val = val_set.groupby('srch_id').size().to_list()\n",
    "else:\n",
    "    X_train = training_sample[features]\n",
    "    y_train = training_sample[target]\n",
    "    group_train = training_sample.groupby('srch_id').size().to_list()\n",
    "\n",
    "# Preprocessing pipeline\n",
    "numeric_transformer = make_pipeline(\n",
    "    SimpleImputer(strategy='mean'),\n",
    "    StandardScaler()\n",
    ")\n",
    "\n",
    "categorical_transformer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "preprocessing = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Fit and transform training data\n",
    "X_train_preprocessed = preprocessing.fit_transform(X_train)\n",
    "\n",
    "if use_validation:\n",
    "    X_val_preprocessed = preprocessing.transform(X_val)\n",
    "\n",
    "# LightGBM model\n",
    "lgbm_model = LGBMRanker(\n",
    "    objective='lambdarank',\n",
    "    metric='ndcg',\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=63,\n",
    "    max_depth=7,\n",
    "    min_data_in_leaf=50,\n",
    "    n_estimators=500,\n",
    "    feature_fraction=0.8,\n",
    "    bagging_fraction=0.8,\n",
    "    bagging_freq=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "if use_validation:\n",
    "    lgbm_model.fit(\n",
    "        X_train_preprocessed, y_train,\n",
    "        group=group_train,\n",
    "        eval_set=[(X_val_preprocessed, y_val)],\n",
    "        eval_group=[group_val],\n",
    "        eval_at=[1, 5, 10],\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=50)],\n",
    "        verbose=True\n",
    "    )\n",
    "else:\n",
    "    lgbm_model.fit(X_train_preprocessed, y_train, group=group_train)\n",
    "\n",
    "X_test = test_set[features]\n",
    "srch_ids = test_set['srch_id']\n",
    "prop_ids = test_set['prop_id']\n",
    "\n",
    "X_test_preprocessed = preprocessing.transform(X_test)\n",
    "scores = lgbm_model.predict(X_test_preprocessed, num_iteration=lgbm_model.best_iteration_)\n",
    "\n",
    "output_df = pd.DataFrame({\n",
    "    'srch_id': srch_ids,\n",
    "    'prop_id': prop_ids,\n",
    "    'score': scores\n",
    "})\n",
    "\n",
    "output_df = output_df.sort_values(by=['srch_id', 'score'], ascending=[True, False])\n",
    "output_df[['srch_id', 'prop_id']].to_csv('ranked_predictions.csv', index=False)\n",
    "print(\"Predictions saved to ranked_predictions.csv\")\n",
    "print(f\"Best iteration: {lgbm_model.best_iteration_}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b0a4a6",
   "metadata": {},
   "source": [
    "Kaggle allows for only 4 submissions per day. If we evaluate locally we don't have to submit as often."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b4a54c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.212474 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4069\n",
      "[LightGBM] [Info] Number of data points in the train set: 3966682, number of used features: 54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/krisstallenberg/anaconda3/envs/recommender-expedia/lib/python3.10/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRanker was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation NDCG@5: 0.3847\n",
      "Validation MRR: 0.3680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/krisstallenberg/anaconda3/envs/recommender-expedia/lib/python3.10/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRanker was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to ranked_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from lightgbm import LGBMRanker\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "# Parameters\n",
    "training_fraction = 1\n",
    "use_validation = True  # Enable validation split\n",
    "\n",
    "# Define features and target\n",
    "features = ['month', 'day', 'year', 'hour', 'season', 'day_of_week', 'is_weekend', 'site_id', 'visitor_location_country_id',\n",
    "            'visitor_hist_starrating', 'visitor_hist_adr_usd', 'prop_country_id', 'prop_starrating', 'prop_review_score',\n",
    "            'prop_brand_bool', 'prop_location_score1', 'prop_location_score2', 'prop_log_historical_price', 'price_usd',\n",
    "            'promotion_flag','srch_destination_id', 'srch_length_of_stay', 'srch_booking_window', 'srch_adults_count',\n",
    "            'srch_children_count', 'srch_room_count', 'srch_saturday_night_bool', 'srch_query_affinity_score',\n",
    "            'orig_destination_distance', 'random_bool', 'comp1_rate', 'comp1_inv', 'comp1_rate_percent_diff',\n",
    "            'comp2_rate', 'comp2_inv', 'comp2_rate_percent_diff', 'comp3_rate', 'comp3_inv', 'comp3_rate_percent_diff',\n",
    "            'comp4_rate', 'comp4_inv', 'comp4_rate_percent_diff', 'comp5_rate', 'comp5_inv', 'comp5_rate_percent_diff',\n",
    "            'comp6_rate', 'comp6_inv', 'comp6_rate_percent_diff', 'comp7_rate', 'comp7_inv', 'comp7_rate_percent_diff',\n",
    "            'comp8_rate', 'comp8_inv', 'comp8_rate_percent_diff']\n",
    "target = 'booking_bool'\n",
    "\n",
    "numeric_features = ['month', 'day', 'year', 'hour', 'visitor_hist_starrating', 'visitor_hist_adr_usd', 'prop_starrating',\n",
    "                    'prop_review_score', 'prop_location_score1', 'prop_location_score2', 'prop_log_historical_price',\n",
    "                    'price_usd', 'srch_length_of_stay', 'srch_booking_window', 'srch_adults_count', 'srch_children_count',\n",
    "                    'srch_room_count', 'srch_query_affinity_score', 'orig_destination_distance']\n",
    "\n",
    "categorical_features = ['season', 'day_of_week', 'is_weekend', 'site_id', 'visitor_location_country_id', 'prop_country_id',\n",
    "                        'prop_brand_bool', 'promotion_flag', 'srch_destination_id', 'srch_saturday_night_bool', 'random_bool',\n",
    "                        'comp1_rate', 'comp1_inv', 'comp1_rate_percent_diff', 'comp2_rate', 'comp2_inv', 'comp2_rate_percent_diff',\n",
    "                        'comp3_rate', 'comp3_inv', 'comp3_rate_percent_diff', 'comp4_rate', 'comp4_inv', 'comp4_rate_percent_diff',\n",
    "                        'comp5_rate', 'comp5_inv', 'comp5_rate_percent_diff', 'comp6_rate', 'comp6_inv', 'comp6_rate_percent_diff',\n",
    "                        'comp7_rate', 'comp7_inv', 'comp7_rate_percent_diff', 'comp8_rate', 'comp8_inv', 'comp8_rate_percent_diff']\n",
    "\n",
    "# Sample data\n",
    "training_sample = training_set.sample(frac=training_fraction, random_state=42)\n",
    "\n",
    "# Create validation split grouped by srch_id\n",
    "if use_validation:\n",
    "    splitter = GroupShuffleSplit(test_size=0.2, n_splits=1, random_state=42)\n",
    "    train_idx, val_idx = next(splitter.split(training_sample, groups=training_sample['srch_id']))\n",
    "    train_set = training_sample.iloc[train_idx]\n",
    "    val_set = training_sample.iloc[val_idx]\n",
    "else:\n",
    "    train_set = training_sample\n",
    "    val_set = None\n",
    "\n",
    "# Prepare training data\n",
    "X_train = train_set[features + ['srch_id']]\n",
    "y_train = train_set[target]\n",
    "group_train = X_train.groupby('srch_id').size().to_list()\n",
    "X_train = X_train.drop(columns=['srch_id'])\n",
    "\n",
    "# Prepare validation data if needed\n",
    "if use_validation:\n",
    "    X_val = val_set[features + ['srch_id']]\n",
    "    y_val = val_set[target]\n",
    "    group_val = X_val.groupby('srch_id').size().to_list()\n",
    "    X_val = X_val.drop(columns=['srch_id'])\n",
    "\n",
    "# Preprocessing pipelines\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "preprocessing = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Model\n",
    "lgbm_model = LGBMRanker(\n",
    "    objective='lambdarank',\n",
    "    metric='ndcg',\n",
    "    learning_rate=0.1,\n",
    "    num_leaves=31,\n",
    "    max_depth=10,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    n_estimators=500\n",
    ")\n",
    "\n",
    "# Pipeline combining preprocessing and model\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessing', preprocessing),\n",
    "    ('lgbm', lgbm_model)\n",
    "])\n",
    "\n",
    "# Fit model\n",
    "model.fit(X_train, y_train, lgbm__group=group_train)\n",
    "\n",
    "# Evaluate on validation set if present\n",
    "if use_validation:\n",
    "    from sklearn.metrics import ndcg_score\n",
    "\n",
    "    X_val_preprocessed = model.named_steps['preprocessing'].transform(X_val)\n",
    "    val_scores = model.named_steps['lgbm'].predict(X_val_preprocessed, num_iteration=model.named_steps['lgbm'].best_iteration_)\n",
    "\n",
    "    val_set_with_scores = val_set.copy()\n",
    "    val_set_with_scores['score'] = val_scores\n",
    "\n",
    "    # Sort within each group by score descending\n",
    "    val_set_with_scores = val_set_with_scores.sort_values(by=['srch_id', 'score'], ascending=[True, False])\n",
    "    grouped_val = val_set_with_scores.groupby('srch_id')\n",
    "\n",
    "    ndcg_scores = []\n",
    "    mrr_scores = []\n",
    "\n",
    "    for _, group in grouped_val:\n",
    "        true_relevance = group[target].values\n",
    "        if np.sum(true_relevance) == 0:\n",
    "            # No relevant items, skip this group\n",
    "            continue\n",
    "\n",
    "        # Compute NDCG@5 (sklearn expects arrays of shape (1, n_items))\n",
    "        ndcg = ndcg_score([true_relevance], [group['score'].values], k=5)\n",
    "        ndcg_scores.append(ndcg)\n",
    "\n",
    "        # Compute MRR: reciprocal rank of first relevant item\n",
    "        ranks = np.where(true_relevance > 0)[0]\n",
    "        if len(ranks) > 0:\n",
    "            mrr_scores.append(1.0 / (ranks[0] + 1))\n",
    "\n",
    "    print(f\"Validation NDCG@5: {np.mean(ndcg_scores):.4f}\")\n",
    "    print(f\"Validation MRR: {np.mean(mrr_scores):.4f}\")\n",
    "\n",
    "# Predict on test set (no target in test set assumed)\n",
    "X_test = test_set[features]\n",
    "srch_ids = test_set['srch_id']\n",
    "prop_ids = test_set['prop_id']\n",
    "\n",
    "test_scores = model.predict(X_test)\n",
    "\n",
    "output_df = pd.DataFrame({\n",
    "    'srch_id': srch_ids,\n",
    "    'prop_id': prop_ids,\n",
    "    'score': test_scores\n",
    "})\n",
    "\n",
    "output_df = output_df.sort_values(by=['srch_id', 'score'], ascending=[True, False])\n",
    "output_df[['srch_id', 'prop_id']].to_csv('ranked_predictions.csv', index=False)\n",
    "print(\"Predictions saved to ranked_predictions.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6acb6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recommender-expedia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
